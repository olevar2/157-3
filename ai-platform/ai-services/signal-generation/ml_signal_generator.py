"""
Machine Learning Signal Generator

This module implements advanced machine learning algorithms for trading signal
generation, including ensemble methods, deep learning features, and multi-class
signal classification with confidence scoring.

Mathematical Foundation:
- Random Forest Ensemble Classification
- Gradient Boosting Signal Prediction
- Feature Engineering and Selection
- Multi-timeframe Signal Fusion
- Confidence Scoring with Calibration
- Online Learning and Adaptation

Author: Platform3 Trading System
Version: 1.0.0 - AI Enhancement
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import warnings
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import cross_val_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import accuracy_score, precision_score, recall_score
from ..indicator_base import IndicatorBase

warnings.filterwarnings('ignore')

@dataclass
class MLSignal:
    """Signal generated by machine learning algorithms."""
    timestamp: datetime
    signal_class: str  # 'strong_buy', 'buy', 'hold', 'sell', 'strong_sell'
    confidence: float
    feature_importance: Dict[str, float]
    model_consensus: Dict[str, str]
    prediction_probability: Dict[str, float]
    signal_strength: float
    risk_score: float
    expected_return: float

@dataclass
class ModelMetrics:
    """Performance metrics for ML models."""
    model_name: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    calibration_score: float
    feature_count: int

class MLSignalGenerator(IndicatorBase):
    """
    Machine Learning Signal Generator
    
    Uses ensemble machine learning methods to generate trading signals
    with confidence scoring and feature importance analysis.
    """
    
    def __init__(self, 
                 lookback_period: int = 100,
                 feature_window: int = 20,
                 prediction_horizon: int = 5,
                 min_training_samples: int = 200,
                 retrain_frequency: int = 50,
                 confidence_threshold: float = 0.6):
        """
        Initialize ML Signal Generator.
        
        Args:
            lookback_period: Period for signal generation
            feature_window: Window for feature calculation
            prediction_horizon: Horizon for signal prediction
            min_training_samples: Minimum samples for training
            retrain_frequency: Frequency of model retraining
            confidence_threshold: Minimum confidence for signals
        """
        super().__init__(lookback_period)
        self.feature_window = feature_window
        self.prediction_horizon = prediction_horizon
        self.min_training_samples = min_training_samples
        self.retrain_frequency = retrain_frequency
        self.confidence_threshold = confidence_threshold
        
        # ML Models
        self.models = self._initialize_models()
        self.calibrated_models = {}
        self.model_weights = {}
        
        # Feature engineering
        self.scaler = RobustScaler()
        self.feature_names = []
        self.feature_importance_history = []
        
        # Training data
        self.training_features = []
        self.training_labels = []
        self.last_retrain = 0
        
        # Performance tracking
        self.prediction_history = []
        self.accuracy_history = []
        self.model_metrics = {}
        
    def _initialize_models(self) -> Dict[str, Any]:
        """Initialize machine learning models."""
        models = {
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            ),
            'logistic_regression': LogisticRegression(
                C=1.0,
                max_iter=1000,
                random_state=42,
                multi_class='ovr'
            )
        }
        
        # Initialize model weights (equal initially)
        self.model_weights = {name: 1.0/len(models) for name in models.keys()}
        
        return models
    
    def _engineer_features(self, data: pd.DataFrame) -> np.ndarray:
        """Engineer features for machine learning."""
        prices = data['close'].values
        volumes = data['volume'].values if 'volume' in data.columns else np.ones(len(prices))
        highs = data['high'].values
        lows = data['low'].values
        opens = data['open'].values
        
        features = []
        
        for i in range(self.feature_window, len(data)):
            window_prices = prices[i-self.feature_window:i]
            window_volumes = volumes[i-self.feature_window:i]
            window_highs = highs[i-self.feature_window:i]
            window_lows = lows[i-self.feature_window:i]
            window_opens = opens[i-self.feature_window:i]
            
            feature_vector = []
            
            # Price-based features
            returns = np.diff(window_prices) / window_prices[:-1]
            feature_vector.extend([
                np.mean(returns),  # Average return
                np.std(returns),   # Volatility
                np.skew(returns) if len(returns) > 2 else 0.0,  # Skewness
                np.kurtosis(returns) if len(returns) > 3 else 0.0,  # Kurtosis
            ])
            
            # Technical indicators
            feature_vector.extend([
                self._calculate_rsi(window_prices),
                self._calculate_macd_signal(window_prices),
                self._calculate_bollinger_position(window_prices),
                self._calculate_momentum(window_prices, 5),
                self._calculate_momentum(window_prices, 10),
            ])
            
            # Volume features
            feature_vector.extend([
                np.mean(window_volumes),
                np.std(window_volumes),
                np.corrcoef(window_prices, window_volumes)[0, 1] if len(window_prices) > 1 else 0.0,
            ])
            
            # Price action features
            feature_vector.extend([
                (window_prices[-1] - window_prices[0]) / window_prices[0],  # Total return
                (np.max(window_highs) - window_prices[-1]) / window_prices[-1],  # Distance from high
                (window_prices[-1] - np.min(window_lows)) / window_prices[-1],   # Distance from low
                np.mean(window_highs - window_lows) / np.mean(window_prices),    # Average range
            ])
            
            # Trend features
            feature_vector.extend([
                self._calculate_trend_strength(window_prices),
                self._calculate_support_resistance_strength(window_prices),
                self._calculate_breakout_probability(window_prices, window_volumes),
            ])
            
            # Multi-timeframe features
            if i >= self.feature_window * 2:
                longer_window = prices[i-self.feature_window*2:i]
                feature_vector.extend([
                    self._calculate_momentum(longer_window, 10),
                    self._calculate_trend_strength(longer_window),
                ])
            else:
                feature_vector.extend([0.0, 0.0])
            
            features.append(feature_vector)
        
        # Store feature names for interpretability
        self.feature_names = [
            'avg_return', 'volatility', 'skewness', 'kurtosis',
            'rsi', 'macd_signal', 'bb_position', 'momentum_5', 'momentum_10',
            'avg_volume', 'volume_std', 'price_volume_corr',
            'total_return', 'distance_from_high', 'distance_from_low', 'avg_range',
            'trend_strength', 'support_resistance', 'breakout_prob',
            'long_momentum', 'long_trend'
        ]
        
        return np.array(features)
    
    def _calculate_rsi(self, prices: np.ndarray, period: int = 14) -> float:
        """Calculate RSI."""
        if len(prices) < period + 1:
            return 50.0
        
        deltas = np.diff(prices)
        gains = np.where(deltas > 0, deltas, 0)
        losses = np.where(deltas < 0, -deltas, 0)
        
        avg_gain = np.mean(gains[-period:])
        avg_loss = np.mean(losses[-period:])
        
        if avg_loss == 0:
            return 100.0
        
        rs = avg_gain / avg_loss
        return 100 - (100 / (1 + rs))
    
    def _calculate_macd_signal(self, prices: np.ndarray) -> float:
        """Calculate MACD signal."""
        if len(prices) < 26:
            return 0.0
        
        ema_12 = self._calculate_ema(prices, 12)
        ema_26 = self._calculate_ema(prices, 26)
        macd = ema_12[-1] - ema_26[-1]
        
        return macd / prices[-1]  # Normalize by price
    
    def _calculate_ema(self, prices: np.ndarray, period: int) -> np.ndarray:
        """Calculate EMA."""
        alpha = 2 / (period + 1)
        ema = np.zeros_like(prices, dtype=float)
        ema[0] = prices[0]
        
        for i in range(1, len(prices)):
            ema[i] = alpha * prices[i] + (1 - alpha) * ema[i-1]
        
        return ema
    
    def _calculate_bollinger_position(self, prices: np.ndarray, period: int = 20) -> float:
        """Calculate position within Bollinger Bands."""
        if len(prices) < period:
            return 0.5
        
        sma = np.mean(prices[-period:])
        std = np.std(prices[-period:])
        
        if std == 0:
            return 0.5
        
        upper = sma + 2 * std
        lower = sma - 2 * std
        
        return (prices[-1] - lower) / (upper - lower)
    
    def _calculate_momentum(self, prices: np.ndarray, period: int) -> float:
        """Calculate price momentum."""
        if len(prices) < period + 1:
            return 0.0
        
        return (prices[-1] - prices[-period-1]) / prices[-period-1]
    
    def _calculate_trend_strength(self, prices: np.ndarray) -> float:
        """Calculate trend strength using linear regression."""
        if len(prices) < 5:
            return 0.0
        
        x = np.arange(len(prices))
        coeffs = np.polyfit(x, prices, 1)
        slope = coeffs[0]
        
        return slope / np.mean(prices)
    
    def _calculate_support_resistance_strength(self, prices: np.ndarray) -> float:
        """Calculate support/resistance strength."""
        if len(prices) < 10:
            return 0.0
        
        # Find local extrema
        highs = []
        lows = []
        
        for i in range(1, len(prices) - 1):
            if prices[i] > prices[i-1] and prices[i] > prices[i+1]:
                highs.append(prices[i])
            elif prices[i] < prices[i-1] and prices[i] < prices[i+1]:
                lows.append(prices[i])
        
        if not highs and not lows:
            return 0.0
        
        # Calculate strength as consistency of levels
        all_levels = highs + lows
        level_std = np.std(all_levels) if len(all_levels) > 1 else 0.0
        level_mean = np.mean(all_levels) if len(all_levels) > 0 else prices[-1]
        
        return 1.0 / (1.0 + level_std / level_mean) if level_mean > 0 else 0.0
    
    def _calculate_breakout_probability(self, prices: np.ndarray, volumes: np.ndarray) -> float:
        """Calculate breakout probability."""
        if len(prices) < 10:
            return 0.5
        
        # Volume surge indicator
        recent_volume = np.mean(volumes[-5:])
        avg_volume = np.mean(volumes[:-5])
        volume_ratio = recent_volume / (avg_volume + 1e-8)
        
        # Price compression indicator
        recent_range = np.max(prices[-5:]) - np.min(prices[-5:])
        avg_range = np.mean([np.max(prices[i:i+5]) - np.min(prices[i:i+5]) 
                           for i in range(len(prices)-10, len(prices)-5)])
        range_ratio = recent_range / (avg_range + 1e-8)
        
        # Combine indicators
        breakout_prob = (volume_ratio * 0.6 + (1/range_ratio) * 0.4) / 2
        
        return min(breakout_prob, 1.0)
    
    def _create_labels(self, prices: np.ndarray) -> np.ndarray:
        """Create labels for supervised learning."""
        labels = []
        
        for i in range(len(prices) - self.prediction_horizon):
            current_price = prices[i]
            future_price = prices[i + self.prediction_horizon]
            
            return_pct = (future_price - current_price) / current_price
            
            # Multi-class classification
            if return_pct > 0.02:  # 2% gain
                labels.append(4)  # Strong buy
            elif return_pct > 0.005:  # 0.5% gain
                labels.append(3)  # Buy
            elif return_pct > -0.005:  # Between -0.5% and 0.5%
                labels.append(2)  # Hold
            elif return_pct > -0.02:  # Between -2% and -0.5%
                labels.append(1)  # Sell
            else:  # Less than -2%
                labels.append(0)  # Strong sell
        
        return np.array(labels)
    
    def _train_models(self, features: np.ndarray, labels: np.ndarray):
        """Train machine learning models."""
        if len(features) < self.min_training_samples:
            return
        
        # Scale features
        features_scaled = self.scaler.fit_transform(features)
        
        # Train each model
        for name, model in self.models.items():
            try:
                # Train model
                model.fit(features_scaled, labels)
                
                # Create calibrated version
                self.calibrated_models[name] = CalibratedClassifierCV(
                    model, method='isotonic', cv=3
                )
                self.calibrated_models[name].fit(features_scaled, labels)
                
                # Calculate performance metrics
                predictions = model.predict(features_scaled)
                accuracy = accuracy_score(labels, predictions)
                
                # Update model weights based on performance
                self.model_weights[name] = accuracy
                
                # Store metrics
                self.model_metrics[name] = ModelMetrics(
                    model_name=name,
                    accuracy=accuracy,
                    precision=precision_score(labels, predictions, average='weighted', zero_division=0),
                    recall=recall_score(labels, predictions, average='weighted', zero_division=0),
                    f1_score=2 * (precision_score(labels, predictions, average='weighted', zero_division=0) * 
                                recall_score(labels, predictions, average='weighted', zero_division=0)) / 
                            (precision_score(labels, predictions, average='weighted', zero_division=0) + 
                             recall_score(labels, predictions, average='weighted', zero_division=0) + 1e-8),
                    calibration_score=0.0,  # Simplified
                    feature_count=len(self.feature_names)
                )
                
            except Exception as e:
                print(f"Error training model {name}: {e}")
                self.model_weights[name] = 0.0
        
        # Normalize weights
        total_weight = sum(self.model_weights.values())
        if total_weight > 0:
            self.model_weights = {k: v/total_weight for k, v in self.model_weights.items()}
    
    def _predict_ensemble(self, features: np.ndarray) -> Tuple[str, float, Dict[str, float]]:
        """Make ensemble prediction."""
        if len(self.calibrated_models) == 0:
            return 'hold', 0.5, {}
        
        features_scaled = self.scaler.transform(features.reshape(1, -1))
        
        # Get predictions from all models
        predictions = {}
        probabilities = {}
        
        for name, model in self.calibrated_models.items():
            try:
                pred = model.predict(features_scaled)[0]
                prob = model.predict_proba(features_scaled)[0]
                
                predictions[name] = pred
                probabilities[name] = np.max(prob)
                
            except Exception as e:
                print(f"Error predicting with model {name}: {e}")
                predictions[name] = 2  # Default to hold
                probabilities[name] = 0.5
        
        # Weighted ensemble
        class_votes = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
        total_weight = 0
        
        for name, pred in predictions.items():
            weight = self.model_weights.get(name, 0)
            class_votes[pred] += weight
            total_weight += weight
        
        # Get final prediction
        if total_weight > 0:
            final_class = max(class_votes, key=class_votes.get)
        else:
            final_class = 2  # Hold
        
        # Calculate confidence
        confidence = class_votes[final_class] / (total_weight + 1e-8)
        
        # Map class to signal
        class_names = ['strong_sell', 'sell', 'hold', 'buy', 'strong_buy']
        signal = class_names[final_class]
        
        return signal, confidence, probabilities
    
    def _calculate_feature_importance(self) -> Dict[str, float]:
        """Calculate feature importance across models."""
        importance_dict = {}
        
        for name, model in self.models.items():
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
                weight = self.model_weights.get(name, 0)
                
                for i, importance in enumerate(importances):
                    feature_name = self.feature_names[i] if i < len(self.feature_names) else f'feature_{i}'
                    if feature_name not in importance_dict:
                        importance_dict[feature_name] = 0
                    importance_dict[feature_name] += importance * weight
        
        return importance_dict
    
    def calculate(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate ML-generated signals.
        
        Args:
            data: DataFrame with OHLCV data
            
        Returns:
            DataFrame with ML signals
        """
        if len(data) < self.period + self.feature_window:
            return pd.DataFrame()
        
        # Engineer features
        features = self._engineer_features(data)
        
        if len(features) == 0:
            return pd.DataFrame()
        
        # Create labels for training
        prices = data['close'].values
        window_prices = prices[self.feature_window:]
        labels = self._create_labels(window_prices)
        
        # Adjust features and labels to match
        min_length = min(len(features), len(labels))
        features = features[:min_length]
        labels = labels[:min_length]
        
        results = []
        
        for i in range(self.period, len(features)):
            # Retrain models periodically
            if (i - self.last_retrain) >= self.retrain_frequency:
                train_features = features[max(0, i-self.min_training_samples):i]
                train_labels = labels[max(0, i-self.min_training_samples):i]
                
                if len(train_features) >= self.min_training_samples:
                    self._train_models(train_features, train_labels)
                    self.last_retrain = i
            
            # Make prediction
            if len(self.calibrated_models) > 0:
                signal, confidence, model_probs = self._predict_ensemble(features[i])
                
                # Calculate additional metrics
                feature_importance = self._calculate_feature_importance()
                
                # Calculate signal strength
                signal_strength = confidence
                if signal in ['strong_buy', 'strong_sell']:
                    signal_strength *= 1.5
                elif signal in ['buy', 'sell']:
                    signal_strength *= 1.2
                
                # Calculate risk score (inverse of confidence for extreme signals)
                if signal in ['strong_buy', 'strong_sell']:
                    risk_score = 1.0 - confidence
                else:
                    risk_score = 0.5
                
                # Expected return (simplified)
                expected_return = 0.0
                if signal == 'strong_buy':
                    expected_return = 0.02 * confidence
                elif signal == 'buy':
                    expected_return = 0.01 * confidence
                elif signal == 'sell':
                    expected_return = -0.01 * confidence
                elif signal == 'strong_sell':
                    expected_return = -0.02 * confidence
                
                # Model consensus
                model_consensus = {}
                class_names = ['strong_sell', 'sell', 'hold', 'buy', 'strong_buy']
                for name, model in self.calibrated_models.items():
                    try:
                        pred = model.predict(self.scaler.transform(features[i].reshape(1, -1)))[0]
                        model_consensus[name] = class_names[pred]
                    except:
                        model_consensus[name] = 'hold'
                
                # Prediction probabilities
                pred_probs = {}
                for j, class_name in enumerate(class_names):
                    pred_probs[class_name] = 0.0
                
                for name, model in self.calibrated_models.items():
                    try:
                        probs = model.predict_proba(self.scaler.transform(features[i].reshape(1, -1)))[0]
                        weight = self.model_weights.get(name, 0)
                        for j, prob in enumerate(probs):
                            pred_probs[class_names[j]] += prob * weight
                    except:
                        pass
                
            else:
                signal = 'hold'
                confidence = 0.5
                feature_importance = {}
                model_consensus = {}
                pred_probs = {'hold': 1.0}
                signal_strength = 0.5
                risk_score = 0.5
                expected_return = 0.0
            
            # Create result
            result = {
                'timestamp': data.index[i + self.feature_window],
                'signal_class': signal,
                'confidence': confidence,
                'signal_strength': signal_strength,
                'risk_score': risk_score,
                'expected_return': expected_return,
                'model_count': len(self.calibrated_models),
                'feature_count': len(feature_importance),
                'consensus_strength': len([s for s in model_consensus.values() if s == signal]) / max(len(model_consensus), 1),
                'top_feature': max(feature_importance.items(), key=lambda x: x[1])[0] if feature_importance else 'none',
                'signal_numeric': ['strong_sell', 'sell', 'hold', 'buy', 'strong_buy'].index(signal)
            }
            
            results.append(result)
        
        return pd.DataFrame(results)
    
    def get_signals(self, data: pd.DataFrame) -> List[MLSignal]:
        """Get ML signals."""
        df = self.calculate(data)
        signals = []
        
        for _, row in df.iterrows():
            signal = MLSignal(
                timestamp=row['timestamp'],
                signal_class=row['signal_class'],
                confidence=row['confidence'],
                feature_importance={},  # Simplified for return
                model_consensus={},     # Simplified for return
                prediction_probability={'signal': row['confidence']},
                signal_strength=row['signal_strength'],
                risk_score=row['risk_score'],
                expected_return=row['expected_return']
            )
            signals.append(signal)
        
        return signals
    
    def get_model_analysis(self) -> Dict[str, Any]:
        """Get comprehensive model analysis."""
        analysis = {
            'model_metrics': {name: {
                'accuracy': metrics.accuracy,
                'precision': metrics.precision,
                'recall': metrics.recall,
                'f1_score': metrics.f1_score
            } for name, metrics in self.model_metrics.items()},
            'model_weights': self.model_weights,
            'feature_importance': self._calculate_feature_importance(),
            'training_samples': len(self.training_features),
            'prediction_accuracy': np.mean(self.accuracy_history) if self.accuracy_history else 0.0
        }
        
        return analysis
    
    def __str__(self) -> str:
        return f"MLSignalGenerator(models={len(self.models)}, period={self.period})"
