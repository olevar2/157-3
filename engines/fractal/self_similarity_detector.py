# -*- coding: utf-8 -*-

# Platform3 path management
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / "shared"))
sys.path.append(str(project_root / "engines"))

"""
Self-Similarity Pattern Detection Indicator

This module implements advanced self-similarity detection for financial markets,
identifying recursive patterns that repeat across multiple scales. Self-similarity
is a fundamental property of fractal structures and market behavior.

Mathematical Foundation:
- Hurst Exponent Analysis for long-range dependence
- Fractal Dimension Correlation for scale invariance
- Pattern Matching across multiple timeframes
- Statistical Self-Similarity Measures
- Recursive Pattern Identification
- Scale-Invariant Feature Detection

Author: Platform3 Trading System
Version: 1.0.0 - Final Implementation
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import warnings
from scipy import stats
from scipy.signal import correlate, find_peaks
from sklearn.metrics import mutual_info_score
from engines.indicator_base import IndicatorBase

warnings.filterwarnings('ignore')

@dataclass
class SelfSimilaritySignal:
    """Signal generated by self-similarity detection."""
    timestamp: datetime
    similarity_score: float
    pattern_strength: float
    scale_invariance: float
    recursive_depth: int
    pattern_type: str
    confidence: float
    timeframe_correlation: float
    fractal_signature: Dict[str, float]

@dataclass
class PatternSignature:
    """Signature of a detected self-similar pattern."""
    pattern_id: str
    base_scale: int
    matching_scales: List[int]
    correlation_scores: List[float]
    statistical_significance: float
    pattern_vector: np.ndarray

class SelfSimilarityDetector(IndicatorBase):
    """
    Advanced Self-Similarity Pattern Detection Indicator
    
    Detects recursive patterns that repeat across multiple scales,
    indicating self-similar fractal behavior in market data.
    """
    
    def __init__(self, 
                 config: Optional[Dict[str, Any]] = None, # Added config
                 period: int = 50,
                 min_pattern_length: int = 5,
                 max_pattern_length: int = 20,
                 similarity_threshold: float = 0.7,
                 scale_range: Tuple[int, int] = (2, 10),
                 significance_level: float = 0.05):
        """
        Initialize Self-Similarity Detector.
        
        Args:
            config: Configuration dictionary.
            period: Lookback period for analysis
            min_pattern_length: Minimum pattern length to detect
            max_pattern_length: Maximum pattern length to detect
            similarity_threshold: Minimum correlation for pattern match
            scale_range: Range of scales to analyze (min_scale, max_scale)
            significance_level: Statistical significance threshold
        """
        super().__init__(config=config) # Pass config to super
        self.period = period # Ensure period is set
        self.min_pattern_length = min_pattern_length
        self.max_pattern_length = max_pattern_length
        self.similarity_threshold = similarity_threshold
        self.min_scale, self.max_scale = scale_range
        self.significance_level = significance_level
        
        # Pattern storage
        self.detected_patterns: List[PatternSignature] = []
        self.pattern_cache: Dict[str, PatternSignature] = {}
        
        # Analysis arrays
        self.similarity_scores = np.array([])
        self.pattern_strengths = np.array([])
        self.scale_invariance = np.array([])
        self.recursive_depths = np.array([])
        
    def _normalize_series(self, series: np.ndarray) -> np.ndarray:
        """Normalize series to zero mean and unit variance."""
        if len(series) < 2:
            return series
        return (series - np.mean(series)) / (np.std(series) + 1e-8)
    
    def _extract_patterns(self, data: np.ndarray, pattern_length: int) -> List[np.ndarray]:
        """Extract all possible patterns of given length from data."""
        patterns = []
        for i in range(len(data) - pattern_length + 1):
            pattern = data[i:i + pattern_length]
            patterns.append(self._normalize_series(pattern))
        return patterns
    
    def _calculate_pattern_correlation(self, pattern1: np.ndarray, pattern2: np.ndarray) -> float:
        """Calculate correlation between two patterns."""
        if len(pattern1) != len(pattern2) or len(pattern1) < 2:
            return 0.0
        
        try:
            correlation = np.corrcoef(pattern1, pattern2)[0, 1]
            return abs(correlation) if not np.isnan(correlation) else 0.0
        except:
            return 0.0
    
    def _find_self_similar_patterns(self, data: np.ndarray) -> List[PatternSignature]:
        """Find self-similar patterns in the data."""
        patterns = []
        
        for pattern_length in range(self.min_pattern_length, 
                                   min(self.max_pattern_length + 1, len(data) // 4)):
            
            # Extract all patterns of this length
            extracted_patterns = self._extract_patterns(data, pattern_length)
            
            if len(extracted_patterns) < 2:
                continue
            
            # Find similar patterns
            for i, base_pattern in enumerate(extracted_patterns[:-1]):
                matching_patterns = []
                correlation_scores = []
                
                for j, compare_pattern in enumerate(extracted_patterns[i+1:], i+1):
                    correlation = self._calculate_pattern_correlation(base_pattern, compare_pattern)
                    
                    if correlation > self.similarity_threshold:
                        matching_patterns.append(j)
                        correlation_scores.append(correlation)
                
                # If we found similar patterns, create signature
                if len(matching_patterns) >= 1:
                    # Calculate statistical significance
                    significance = self._calculate_statistical_significance(
                        correlation_scores, len(extracted_patterns)
                    )
                    
                    if significance < self.significance_level:
                        pattern_signature = PatternSignature(
                            pattern_id=f"pattern_{i}_{pattern_length}",
                            base_scale=i,
                            matching_scales=matching_patterns,
                            correlation_scores=correlation_scores,
                            statistical_significance=significance,
                            pattern_vector=base_pattern
                        )
                        patterns.append(pattern_signature)
        
        return patterns
    
    def _calculate_statistical_significance(self, correlations: List[float], 
                                          total_patterns: int) -> float:
        """Calculate statistical significance of pattern matches."""
        if not correlations:
            return 1.0
        
        # Use binomial test for significance
        n_matches = len(correlations)
        expected_prob = self.similarity_threshold
        
        try:
            from scipy.stats import binom
            p_value = 1 - binom.cdf(n_matches - 1, total_patterns, expected_prob)
            return p_value
        except:
            return 0.05  # Default to significance threshold
    
    def _analyze_scale_invariance(self, data: np.ndarray, patterns: List[PatternSignature]) -> float:
        """Analyze scale invariance of detected patterns."""
        if not patterns:
            return 0.0
        
        scale_correlations = []
        
        for pattern in patterns:
            # Calculate correlation across different scales
            base_length = len(pattern.pattern_vector)
            
            for scale in range(self.min_scale, min(self.max_scale + 1, len(data) // base_length)):
                scaled_length = base_length * scale
                if scaled_length > len(data):
                    continue
                
                # Extract scaled patterns
                scaled_patterns = self._extract_patterns(data, scaled_length)
                
                for scaled_pattern in scaled_patterns:
                    # Downsample to match original pattern length
                    downsampled = self._downsample_pattern(scaled_pattern, base_length)
                    correlation = self._calculate_pattern_correlation(
                        pattern.pattern_vector, downsampled
                    )
                    
                    if correlation > self.similarity_threshold * 0.8:  # Slightly relaxed threshold
                        scale_correlations.append(correlation)
        
        return np.mean(scale_correlations) if scale_correlations else 0.0
    
    def _downsample_pattern(self, pattern: np.ndarray, target_length: int) -> np.ndarray:
        """Downsample pattern to target length."""
        if len(pattern) <= target_length:
            return pattern
        
        # Use linear interpolation for downsampling
        indices = np.linspace(0, len(pattern) - 1, target_length)
        return np.interp(indices, np.arange(len(pattern)), pattern)
    
    def _calculate_recursive_depth(self, patterns: List[PatternSignature]) -> int:
        """Calculate maximum recursive depth of patterns."""
        if not patterns:
            return 0
        
        max_depth = 0
        
        for pattern in patterns:
            # Count nested patterns within this pattern
            depth = 1
            for other_pattern in patterns:
                if (other_pattern.base_scale > pattern.base_scale and 
                    other_pattern.base_scale < pattern.base_scale + len(pattern.pattern_vector)):
                    depth += 1
            
            max_depth = max(max_depth, depth)
        
        return max_depth
    
    def _calculate_pattern_strength(self, patterns: List[PatternSignature]) -> float:
        """Calculate overall pattern strength."""
        if not patterns:
            return 0.0
        
        # Combine multiple factors
        avg_correlation = np.mean([
            np.mean(pattern.correlation_scores) 
            for pattern in patterns
        ])
        
        pattern_count_score = min(len(patterns) / 10.0, 1.0)  # Normalize to [0,1]
        
        significance_score = 1.0 - np.mean([
            pattern.statistical_significance 
            for pattern in patterns
        ])
        
        return (avg_correlation * 0.5 + 
                pattern_count_score * 0.3 + 
                significance_score * 0.2)
    
    def _calculate_similarity_score(self, patterns: List[PatternSignature]) -> float:
        """Calculate overall similarity score."""
        if not patterns:
            return 0.0
        
        # Weight by number of matches and correlation strength
        total_score = 0.0
        total_weight = 0.0
        
        for pattern in patterns:
            weight = len(pattern.matching_scales)
            score = np.mean(pattern.correlation_scores)
            total_score += score * weight
            total_weight += weight
        
        return total_score / total_weight if total_weight > 0 else 0.0
    
    def _calculate_fractal_signature(self, data: np.ndarray, 
                                   patterns: List[PatternSignature]) -> Dict[str, float]:
        """Calculate fractal signature of the data."""
        signature = {}
        
        # Pattern density
        signature['pattern_density'] = len(patterns) / len(data) if len(data) > 0 else 0.0
        
        # Average pattern length
        if patterns:
            signature['avg_pattern_length'] = np.mean([
                len(pattern.pattern_vector) for pattern in patterns
            ])
        else:
            signature['avg_pattern_length'] = 0.0
        
        # Correlation distribution
        all_correlations = []
        for pattern in patterns:
            all_correlations.extend(pattern.correlation_scores)
        
        if all_correlations:
            signature['correlation_mean'] = np.mean(all_correlations)
            signature['correlation_std'] = np.std(all_correlations)
            signature['correlation_skew'] = stats.skew(all_correlations)
        else:
            signature['correlation_mean'] = 0.0
            signature['correlation_std'] = 0.0
            signature['correlation_skew'] = 0.0
        
        # Scale distribution
        if patterns:
            scales = [pattern.base_scale for pattern in patterns]
            signature['scale_distribution'] = np.std(scales) / np.mean(scales) if np.mean(scales) > 0 else 0.0
        else:
            signature['scale_distribution'] = 0.0
        
        return signature
    
    def calculate(self, data: pd.Series) -> Optional[SelfSimilaritySignal]:
        """
        Calculate self-similarity measures and detect patterns.
        
        Args:
            data: Input time series data (e.g., close prices)
            
        Returns:
            SelfSimilaritySignal if a significant pattern is found, else None
        """
        if not isinstance(data, pd.Series):
            # Try to convert if it's a DataFrame with a 'close' column
            if isinstance(data, pd.DataFrame) and 'close' in data.columns:
                data = data['close']
            else:
                self.logger.warning("SelfSimilarityDetector: Input data must be a pandas Series or a DataFrame with a 'close' column.")
                return None

        if len(data) < self.period:
            self.logger.warning(f"SelfSimilarityDetector: Data length ({len(data)}) is less than period ({self.period}).")
            return None
        
        # Use data from the lookback period
        series_data = data.iloc[-self.period:].values
        
        results = []
        
        # Use close prices for analysis
        prices = series_data
        
        for i in range(self.period, len(data)):
            window_data = prices[i - self.period:i]
            
            # Find self-similar patterns
            patterns = self._find_self_similar_patterns(window_data)
            
            # Calculate metrics
            similarity_score = self._calculate_similarity_score(patterns)
            pattern_strength = self._calculate_pattern_strength(patterns)
            scale_invariance = self._analyze_scale_invariance(window_data, patterns)
            recursive_depth = self._calculate_recursive_depth(patterns)
            
            # Determine pattern type
            pattern_type = "strong_similarity" if similarity_score > 0.8 else \
                          "moderate_similarity" if similarity_score > 0.6 else \
                          "weak_similarity"
            
            # Calculate confidence
            confidence = (similarity_score * 0.4 + 
                         pattern_strength * 0.3 + 
                         scale_invariance * 0.2 + 
                         min(recursive_depth / 5.0, 1.0) * 0.1)
            
            # Calculate timeframe correlation (simplified)
            timeframe_correlation = scale_invariance
            
            # Generate fractal signature
            fractal_signature = self._calculate_fractal_signature(window_data, patterns)
            
            # Create signal
            signal = SelfSimilaritySignal(
                timestamp=data.index[i],
                similarity_score=similarity_score,
                pattern_strength=pattern_strength,
                scale_invariance=scale_invariance,
                recursive_depth=recursive_depth,
                pattern_type=pattern_type,
                confidence=confidence,
                timeframe_correlation=timeframe_correlation,
                fractal_signature=fractal_signature
            )
            
            results.append({
                'timestamp': signal.timestamp,
                'similarity_score': signal.similarity_score,
                'pattern_strength': signal.pattern_strength,
                'scale_invariance': signal.scale_invariance,
                'recursive_depth': signal.recursive_depth,
                'pattern_type': signal.pattern_type,
                'confidence': signal.confidence,
                'timeframe_correlation': signal.timeframe_correlation,
                'signal_strength': 'strong' if confidence > 0.7 else 'moderate' if confidence > 0.4 else 'weak',
                'pattern_density': fractal_signature.get('pattern_density', 0.0),
                'correlation_mean': fractal_signature.get('correlation_mean', 0.0),
                'scale_distribution': fractal_signature.get('scale_distribution', 0.0)
            })
            
            # Store patterns for future reference
            self.detected_patterns.extend(patterns)
        
        # Update internal arrays
        if results:
            self.similarity_scores = np.array([r['similarity_score'] for r in results])
            self.pattern_strengths = np.array([r['pattern_strength'] for r in results])
            self.scale_invariance = np.array([r['scale_invariance'] for r in results])
            self.recursive_depths = np.array([r['recursive_depth'] for r in results])
        
        return pd.DataFrame(results)
    
    def get_signals(self, data: pd.DataFrame) -> List[SelfSimilaritySignal]:
        """Get self-similarity signals."""
        df = self.calculate(data)
        signals = []
        
        for _, row in df.iterrows():
            signal = SelfSimilaritySignal(
                timestamp=row['timestamp'],
                similarity_score=row['similarity_score'],
                pattern_strength=row['pattern_strength'],
                scale_invariance=row['scale_invariance'],
                recursive_depth=row['recursive_depth'],
                pattern_type=row['pattern_type'],
                confidence=row['confidence'],
                timeframe_correlation=row['timeframe_correlation'],
                fractal_signature={
                    'pattern_density': row['pattern_density'],
                    'correlation_mean': row['correlation_mean'],
                    'scale_distribution': row['scale_distribution']
                }
            )
            signals.append(signal)
        
        return signals
    
    def get_pattern_analysis(self) -> Dict[str, Any]:
        """Get comprehensive pattern analysis."""
        if len(self.detected_patterns) == 0:
            return {}
        
        analysis = {
            'total_patterns': len(self.detected_patterns),
            'avg_correlation': np.mean([
                np.mean(pattern.correlation_scores) 
                for pattern in self.detected_patterns
            ]),
            'pattern_length_distribution': [
                len(pattern.pattern_vector) 
                for pattern in self.detected_patterns
            ],
            'significance_distribution': [
                pattern.statistical_significance 
                for pattern in self.detected_patterns
            ],
            'most_common_pattern_length': self._get_most_common_pattern_length(),
            'strongest_patterns': self._get_strongest_patterns(5)
        }
        
        return analysis
    
    def _get_most_common_pattern_length(self) -> int:
        """Get most common pattern length."""
        lengths = [len(pattern.pattern_vector) for pattern in self.detected_patterns]
        if not lengths:
            return 0
        return max(set(lengths), key=lengths.count)
    
    def _get_strongest_patterns(self, n: int = 5) -> List[Dict]:
        """Get strongest patterns by correlation."""
        pattern_strengths = []
        
        for pattern in self.detected_patterns:
            avg_correlation = np.mean(pattern.correlation_scores)
            pattern_strengths.append({
                'pattern_id': pattern.pattern_id,
                'correlation': avg_correlation,
                'matches': len(pattern.matching_scales),
                'significance': pattern.statistical_significance
            })
        
        # Sort by correlation strength
        pattern_strengths.sort(key=lambda x: x['correlation'], reverse=True)
        
        return pattern_strengths[:n]
    
    def __str__(self) -> str:
        return f"SelfSimilarityDetector(period={self.period}, threshold={self.similarity_threshold})"
