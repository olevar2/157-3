"""
Adaptive Indicators - Self-Adjusting Parameter Indicators

This module implements AI-powered adaptive indicators that automatically adjust
their parameters based on market conditions, volatility, and regime changes.
Uses machine learning to optimize indicator parameters in real-time.

Mathematical Foundation:
- Kalman Filter for parameter adaptation
- Genetic Algorithm optimization
- Volatility-based parameter scaling
- Regime-dependent parameter sets
- Performance feedback loops
- Online learning algorithms

Author: Platform3 Trading System
Version: 1.0.0 - AI Enhancement
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import warnings
from scipy import optimize
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from ..indicator_base import IndicatorBase

warnings.filterwarnings('ignore')

@dataclass
class AdaptiveSignal:
    """Signal generated by adaptive indicators."""
    timestamp: datetime
    indicator_value: float
    optimal_parameters: Dict[str, float]
    adaptation_score: float
    regime_type: str
    volatility_factor: float
    parameter_stability: float
    performance_score: float
    confidence: float

@dataclass
class ParameterSet:
    """Set of optimized parameters for different market conditions."""
    regime: str
    parameters: Dict[str, float]
    performance: float
    stability_score: float
    usage_count: int
    last_updated: datetime

class AdaptiveIndicators(IndicatorBase):
    """
    Adaptive Indicators with AI-Powered Parameter Optimization
      Automatically adjusts indicator parameters based on market conditions,
    volatility, and performance feedback using machine learning algorithms.
    """
    def __init__(self,
                 base_indicator: str = 'RSI',
                 indicator_type: str = 'AI_ADAPTIVE',
                 timeframe: str = '5m',
                 adaptation_period: int = 50,
                 optimization_window: int = 200,
                 performance_memory: int = 100,
                 volatility_sensitivity: float = 0.5,
                 adaptation_rate: float = 0.1):
        """
        Initialize Adaptive Indicators.
        
        Args:
            base_indicator: Base indicator to adapt ('RSI', 'MACD', 'EMA', etc.)
            adaptation_period: Period for parameter updates
            optimization_window: Window for performance evaluation
            performance_memory: Memory length for performance tracking
            volatility_sensitivity: Sensitivity to volatility changes
            adaptation_rate: Rate of parameter adaptation
        """
        super().__init__(f"AdaptiveIndicators_{base_indicator}", indicator_type, timeframe)
        self.base_indicator = base_indicator
        self.adaptation_period = adaptation_period
        self.period = adaptation_period  # Add period attribute for compatibility
        self.optimization_window = optimization_window
        self.performance_memory = performance_memory
        self.volatility_sensitivity = volatility_sensitivity
        self.adaptation_rate = adaptation_rate
        
        # Parameter management
        self.parameter_sets: Dict[str, ParameterSet] = {}
        self.current_parameters = self._get_default_parameters()
        self.parameter_history = []
        
        # Performance tracking
        self.performance_scores = []
        self.adaptation_scores = []
        
        # Pre-computed static values (Task C: Pre-compute Static Operations)
        self.static_computations = {
            'default_volatility': 0.01,
            'regime_thresholds': {'trending_up': 0.02, 'trending_down': -0.02},
            'performance_weights': np.array([0.4, 0.3, 0.2, 0.1]),  # Pre-computed weights
            'adaptation_frequency_map': {
                'trending': 1000,  # Less frequent for trending markets
                'ranging': 500,    # More frequent for ranging markets
                'volatile': 200    # Most frequent for volatile markets
            }
        }
        
        # Pre-allocate commonly used arrays
        self.temp_arrays = {
            'price_changes': np.zeros(50),  # For volatility calculations
            'performance_buffer': np.zeros(20),  # For performance scoring
            'regime_buffer': np.zeros(10)   # For regime detection
        }
        self.kalman_filter = None
        self.genetic_optimizer = None
        self.scaler = StandardScaler()
        
        # Performance optimization caches
        self.regime_cache = {}  # Cache for market regime calculations
        self.volatility_cache = {}  # Cache for volatility calculations
        self.statistics_cache = {}  # Cache for statistical calculations
        self.cache_max_size = 1000  # Maximum cache entries
        
        # Optimization settings
        self.adaptation_frequency = max(1, adaptation_period // 10)  # Adapt every N points
        self.last_adaptation_index = 0
        
        # Rolling statistics pre-computation
        self.rolling_stats_window = 100
        self.pre_computed_stats = {}
        
        # Market state tracking
        self.volatility_states = []
        self.trend_states = []
        self.regime_states = []
        
    def _get_default_parameters(self) -> Dict[str, float]:
        """Get default parameters for the base indicator."""
        defaults = {
            'RSI': {'period': 14, 'overbought': 70, 'oversold': 30},
            'MACD': {'fast': 12, 'slow': 26, 'signal': 9},
            'EMA': {'period': 20, 'multiplier': 2},
            'Bollinger': {'period': 20, 'std_dev': 2},
            'Stochastic': {'k_period': 14, 'd_period': 3, 'overbought': 80, 'oversold': 20}
        }
        return defaults.get(self.base_indicator, {'period': 14})
    
    def _calculate_market_regime(self, data: np.ndarray) -> str:
        """Identify current market regime with caching optimization."""
        if len(data) < 20:
            return 'unknown'
        
        # Create cache key from last 50 data points (or less if not available)
        cache_window = min(50, len(data))
        cache_data = data[-cache_window:]
        cache_key = hash(cache_data.tobytes())
        
        # Check cache first
        if cache_key in self.regime_cache:
            return self.regime_cache[cache_key]
        
        # Calculate regime only if cache miss
        regime = self._compute_market_regime(data)
        
        # Cache the result (with size limit)
        if len(self.regime_cache) >= self.cache_max_size:
            # Remove oldest entries (simple FIFO)
            oldest_key = next(iter(self.regime_cache))
            del self.regime_cache[oldest_key]
        
        self.regime_cache[cache_key] = regime
        return regime
    
    def _compute_market_regime(self, data: np.ndarray) -> str:
        """Actual market regime computation (called only on cache miss)."""        # FAST APPROXIMATION ALGORITHMS (Task D: Alternative Algorithms)
        data_len = len(data)
        
        # Use pre-computed values when possible
        if data_len < 21:
            return 'ranging'  # Quick exit for insufficient data
        
        # P2.2.1: Safe numerical operations with exception handling
        try:
            # Fast volatility approximation (no expensive std calculation)
            last_prices = data[-11:]  # Use only last 10 prices instead of 21
            
            # Safe min/max operations
            price_max = self._safe_numerical_operation(np.max, last_prices, fallback_value=last_prices[-1], operation_name="price_max")
            price_min = self._safe_numerical_operation(np.min, last_prices, fallback_value=last_prices[-1], operation_name="price_min")
            price_mean = self._safe_numerical_operation(np.mean, last_prices, fallback_value=last_prices[-1], operation_name="price_mean")
            
            # Safe volatility calculation with division by zero protection
            if price_mean > 0:
                fast_volatility = (price_max - price_min) / price_mean
            else:
                fast_volatility = 0.01  # Safe fallback
            
            # Safe trend approximation with bounds checking
            if data_len >= 6 and data[-6] > 0:
                trend_approx = (data[-1] - data[-6]) / data[-6]
            else:
                trend_approx = 0.0  # Safe fallback
            
            # Use pre-computed thresholds from static computations
            thresholds = self.static_computations['regime_thresholds']
            
            # Simple classification (faster than multiple conditions)
            if fast_volatility > 0.02:  # 2% volatility threshold
                return 'volatile'
            elif trend_approx > thresholds['trending_up']:
                return 'trending'
            elif trend_approx < thresholds['trending_down']:
                return 'declining'
            else:
                return 'ranging'
                
        except Exception:
            # P2.2.2: Graceful degradation - return safe default
            return 'ranging'
    
    def _calculate_volatility_factor(self, data: np.ndarray) -> float:
        """Calculate volatility factor for parameter scaling with caching."""
        if len(data) < 10:
            return 1.0
        
        # Create cache key from last 20 data points
        cache_window = min(20, len(data))
        cache_data = data[-cache_window:]
        cache_key = hash(cache_data.tobytes())
        
        # Check cache first
        if cache_key in self.volatility_cache:
            return self.volatility_cache[cache_key]
        
        # Calculate volatility only if cache miss
        returns = np.diff(cache_data) / cache_data[:-1]
        current_vol = np.std(returns)
        
        # Calculate baseline volatility using a fixed window for consistency
        if len(data) >= 100:
            baseline_returns = np.diff(data[-100:-80]) / data[-100:-80][:-1] # Corrected indexing
            baseline_vol = np.std(baseline_returns)
        else:
            baseline_vol = current_vol # Corrected assignment
        
        volatility_factor = current_vol / baseline_vol if baseline_vol > 0 else 1.0
        
        # Cache the result
        if len(self.volatility_cache) >= self.cache_max_size:
            oldest_key = next(iter(self.volatility_cache))
            del self.volatility_cache[oldest_key]
        
        self.volatility_cache[cache_key] = volatility_factor
        return volatility_factor
    
    def _optimize_parameters(self, data: np.ndarray, regime: str) -> Dict[str, float]:
        """Optimize parameters using genetic algorithm approach."""
        if len(data) < self.optimization_window:
            return self.current_parameters
        
        optimization_data = data[-self.optimization_window:]
        
        def objective_function(params):
            """Objective function for parameter optimization."""
            try:
                # Calculate indicator with given parameters
                indicator_values = self._calculate_base_indicator(optimization_data, params)
                
                # Calculate performance score
                performance = self._calculate_performance_score(
                    optimization_data, indicator_values
                )
                
                return -performance  # Minimize negative performance
            except:
                return 1000  # Penalty for invalid parameters
        
        # Define parameter bounds based on base indicator
        bounds = self._get_parameter_bounds()
        
        # Use differential evolution for global optimization
        try:
            result = optimize.differential_evolution(
                objective_function,
                bounds,
                maxiter=50,
                popsize=10,
                seed=42
            )
            
            if result.success:
                optimized_params = self._params_array_to_dict(result.x)
                return optimized_params
        except:
            pass
        
        return self.current_parameters
    
    def _get_parameter_bounds(self) -> List[Tuple[float, float]]:
        """Get parameter bounds for optimization."""
        if self.base_indicator == 'RSI':
            return [(5, 50), (60, 90), (10, 40)]  # period, overbought, oversold
        elif self.base_indicator == 'MACD':
            return [(5, 25), (15, 50), (3, 20)]  # fast, slow, signal
        elif self.base_indicator == 'EMA':
            return [(5, 100), (1, 5)]  # period, multiplier
        elif self.base_indicator == 'Bollinger':
            return [(10, 50), (1, 4)]  # period, std_dev
        else:
            return [(5, 50)]  # generic period
    
    def _params_array_to_dict(self, params_array: np.ndarray) -> Dict[str, float]:
        """Convert parameter array to dictionary."""
        if self.base_indicator == 'RSI':
            return {
                'period': int(params_array[0]),
                'overbought': params_array[1],
                'oversold': params_array[2]
            }
        elif self.base_indicator == 'MACD':
            return {
                'fast': int(params_array[0]),
                'slow': int(params_array[1]),
                'signal': int(params_array[2])
            }
        elif self.base_indicator == 'EMA':
            return {
                'period': int(params_array[0]),
                'multiplier': params_array[1]
            }
        elif self.base_indicator == 'Bollinger':
            return {
                'period': int(params_array[0]),
                'std_dev': params_array[1]
            }
        else:
            return {'period': int(params_array[0])}
    
    def _calculate_base_indicator(self, data: np.ndarray, params: Dict[str, float]) -> np.ndarray:
        """Calculate base indicator with aggressive caching."""
        # Create cache key from indicator type, data length, and key parameters
        cache_key = f"{self.base_indicator}_{len(data)}_{params.get('period', 14)}"
        
        # Check cache first
        if hasattr(self, 'base_indicator_cache') and cache_key in self.base_indicator_cache:
            return self.base_indicator_cache[cache_key]
        
        # Calculate indicator
        if self.base_indicator == 'RSI':
            result = self._calculate_rsi(data, int(params['period']))
        elif self.base_indicator == 'MACD':
            result = self._calculate_macd(data, int(params['fast']), 
                                      int(params['slow']), int(params['signal']))
        elif self.base_indicator == 'EMA':
            result = self._calculate_ema(data, int(params['period']))
        elif self.base_indicator == 'Bollinger':
            bb_upper, bb_middle, bb_lower = self._calculate_bollinger_bands(
                data, int(params['period']), params['std_dev']
            )
            result = bb_middle  # Return middle band as representative value
        else:
            # Default to simple moving average
            result = self._calculate_sma(data, int(params.get('period', 14)))
        
        # Cache result
        if not hasattr(self, 'base_indicator_cache'):
            self.base_indicator_cache = {}
        
        # Limit cache size
        if len(self.base_indicator_cache) > 100:
            old_keys = list(self.base_indicator_cache.keys())[:20]
            for key in old_keys:
                del self.base_indicator_cache[key]
        
        self.base_indicator_cache[cache_key] = result
        return result
    
    def _calculate_rsi(self, data: np.ndarray, period: int) -> np.ndarray:
        """Calculate RSI with given period."""
        if len(data) < period + 1:
            return np.full(len(data), 50.0)
        
        deltas = np.diff(data)
        gains = np.where(deltas > 0, deltas, 0)
        losses = np.where(deltas < 0, -deltas, 0)
        
        avg_gains = np.convolve(gains, np.ones(period)/period, mode='valid')
        avg_losses = np.convolve(losses, np.ones(period)/period, mode='valid')
        
        rs = avg_gains / (avg_losses + 1e-8)
        rsi = 100 - (100 / (1 + rs))
        
        # Pad to match input length
        return np.concatenate([np.full(len(data) - len(rsi), 50.0), rsi])
    
    def _calculate_macd(self, data: np.ndarray, fast: int, slow: int, signal: int) -> np.ndarray:
        """Calculate MACD line."""
        if len(data) < slow:
            return np.zeros(len(data))
        
        ema_fast = self._calculate_ema(data, fast)
        ema_slow = self._calculate_ema(data, slow)
        macd_line = ema_fast - ema_slow
        
        return macd_line
    
    def _calculate_ema(self, data: np.ndarray, period: int) -> np.ndarray:
        """Calculate EMA."""
        if len(data) == 0:
            return np.array([])
        
        alpha = 2 / (period + 1)
        ema = np.zeros_like(data, dtype=float)
        ema[0] = data[0]
        
        for i in range(1, len(data)):
            ema[i] = alpha * data[i] + (1 - alpha) * ema[i-1]        
        return ema
    
    def _calculate_sma(self, data: np.ndarray, period: int) -> np.ndarray:
        """Calculate Simple Moving Average with aggressive caching."""
        # Create cache key from data length and period
        cache_key = f"sma_{len(data)}_{period}"
        
        # Check cache first
        if hasattr(self, 'sma_cache') and cache_key in self.sma_cache:
            return self.sma_cache[cache_key]
        
        if len(data) < period:
            mean_val = np.mean(data)
            result = np.full(len(data), mean_val)
        else:
            # Use optimized numpy convolution
            sma = np.convolve(data, np.ones(period)/period, mode='valid')
            initial_mean = np.mean(data[:period])
            result = np.concatenate([np.full(period-1, initial_mean), sma])
        
        # Cache result
        if not hasattr(self, 'sma_cache'):
            self.sma_cache = {}
        
        # Limit cache size to prevent memory bloat
        if len(self.sma_cache) > 200:
            # Remove oldest entries
            old_keys = list(self.sma_cache.keys())[:50]
            for key in old_keys:
                del self.sma_cache[key]
        
        self.sma_cache[cache_key] = result
        return result
    
    def _calculate_bollinger_bands(self, data: np.ndarray, period: int, std_dev: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Calculate Bollinger Bands."""
        sma = self._calculate_sma(data, period)
        
        if len(data) < period:
            std = np.std(data)
            return sma + std_dev * std, sma, sma - std_dev * std
        
        rolling_std = np.array([
            np.std(data[max(0, i-period+1):i+1]) 
            for i in range(len(data))
        ])
        
        upper = sma + std_dev * rolling_std
        lower = sma - std_dev * rolling_std
        
        return upper, sma, lower
    
    def _calculate_performance_score(self, price_data: np.ndarray, 
                                   indicator_values: np.ndarray) -> float:
        """Calculate performance score for parameter optimization."""
        if len(price_data) != len(indicator_values) or len(price_data) < 10:
            return 0.0
        
        # Calculate returns
        returns = np.diff(price_data) / price_data[:-1]
        
        # Generate signals based on indicator
        signals = self._generate_signals_from_indicator(indicator_values)
        
        if len(signals) != len(returns):
            signals = signals[:len(returns)]
        
        # Calculate strategy returns
        strategy_returns = returns * signals
        
        # Performance metrics
        total_return = np.sum(strategy_returns)
        volatility = np.std(strategy_returns)
        sharpe_ratio = total_return / (volatility + 1e-8) * np.sqrt(252)  # Annualized
        
        # Win rate
        winning_trades = np.sum(strategy_returns > 0)
        total_trades = np.sum(np.abs(signals) > 0)
        win_rate = winning_trades / (total_trades + 1e-8)
        
        # Combined performance score
        performance = (
            sharpe_ratio * 0.4 +
            total_return * 1000 * 0.3 +  # Scale return
            win_rate * 0.3
        )
        
        return performance
    
    def _generate_signals_from_indicator(self, indicator_values: np.ndarray) -> np.ndarray:
        """Generate trading signals from indicator values."""
        signals = np.zeros(len(indicator_values))
        
        if self.base_indicator == 'RSI':
            # RSI-based signals
            overbought = self.current_parameters.get('overbought', 70)
            oversold = self.current_parameters.get('oversold', 30)
            
            signals[indicator_values > overbought] = -1  # Sell
            signals[indicator_values < oversold] = 1     # Buy
            
        elif self.base_indicator == 'MACD':
            # MACD crossover signals
            for i in range(1, len(indicator_values)):
                if indicator_values[i] > 0 and indicator_values[i-1] <= 0:
                    signals[i] = 1  # Buy
                elif indicator_values[i] < 0 and indicator_values[i-1] >= 0:
                    signals[i] = -1  # Sell
        
        else:
            # Generic momentum signals
            for i in range(1, len(indicator_values)):
                if indicator_values[i] > indicator_values[i-1]:
                    signals[i] = 1
                elif indicator_values[i] < indicator_values[i-1]:
                    signals[i] = -1
        
        return signals
    
    def _update_parameter_sets(self, regime: str, params: Dict[str, float], 
                             performance: float):
        """Update parameter sets for different regimes."""
        if regime in self.parameter_sets:
            param_set = self.parameter_sets[regime]
            
            # Update with exponential moving average
            alpha = 0.1
            param_set.performance = (alpha * performance + 
                                   (1 - alpha) * param_set.performance)
            param_set.usage_count += 1
            param_set.last_updated = datetime.now()
        else:            # Create new parameter set
            self.parameter_sets[regime] = ParameterSet(
                regime=regime,
                parameters=params.copy(),
                performance=performance,
                stability_score=1.0,
                usage_count=1,
                last_updated=datetime.now()
            )
    
    def _calculate_parameter_stability(self, params: Dict[str, float]) -> float:
        """Calculate stability score of parameters with caching."""
        if len(self.parameter_history) < 5:
            return 1.0
        
        # Create cache key from parameter values
        param_key = str(sorted(params.items()))
        
        # Check cache
        if hasattr(self, 'stability_cache') and param_key in self.stability_cache:
            return self.stability_cache[param_key]
        
        recent_params = self.parameter_history[-5:]
        # Pre-allocate arrays for better performance
        stability_scores = []
        
        for key in params.keys():
            values = np.array([p.get(key, 0) for p in recent_params])
            if len(values) > 1:
                # Use faster variance calculation
                var_val = np.var(values, ddof=0)  # Population variance is faster
                stability = 1.0 / (1.0 + var_val) if var_val > 0 else 1.0
                stability_scores.append(stability)
        
        result = np.mean(stability_scores) if stability_scores else 1.0
        
        # Cache result
        if not hasattr(self, 'stability_cache'):
            self.stability_cache = {}
        
        # Limit cache size
        if len(self.stability_cache) > 50:
            # Remove oldest entries
            old_keys = list(self.stability_cache.keys())[:10]
            for key in old_keys:
                del self.stability_cache[key]
        
        self.stability_cache[param_key] = result
        return result
    
    def calculate(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate adaptive indicators with robust error handling.
        
        Args:
            data: DataFrame with OHLCV data
            
        Returns:
            DataFrame with adaptive indicator signals
        """
        # P2.1.4: Data Validation at Entry Points
        if not self._validate_input_data(data):
            return pd.DataFrame()
        
        # P2.1.1: Comprehensive NaN Detection
        if self._contains_invalid_data(data):
            data = self._clean_input_data(data)
            if data is None or len(data) < self.period:
                return pd.DataFrame()
        
        if len(data) < self.period:
            return pd.DataFrame()
          # EXTREME result caching with multiple strategies
        data_hash = hash(str(data['close'].values[-100:].tobytes()) if len(data) > 100 else str(data['close'].values.tobytes()))
        cache_key = f"calculate_{data_hash}_{len(data)}_{self.period}"
        
        # Initialize extreme cache system
        if not hasattr(self, 'result_cache'):
            self.result_cache = {}
            self.cache_hits = 0
            self.cache_misses = 0
          # Check cache first (extreme optimization)
        if cache_key in self.result_cache:
            self.cache_hits += 1
            return self.result_cache[cache_key].copy()
        
        self.cache_misses += 1
        
        # Pre-allocate results arrays (Data Structure Optimization)
        num_results = len(data) - self.period
        result_arrays = {
            'indicator_value': np.full(num_results, np.nan),
            'adaptation_score': np.full(num_results, np.nan),
            'regime_type': np.full(num_results, 'ranging', dtype=object),
            'volatility_factor': np.full(num_results, 1.0),
            'parameter_stability': np.full(num_results, 1.0),
            'performance_score': np.full(num_results, 0.0),
            'confidence': np.full(num_results, 0.5)
        }
        
        results = []
        prices = data['close'].values
        
        for i in range(self.period, len(data)):
            window_data = prices[:i+1]
              # Ultra aggressive optimization - only calculate every 500 iterations
            mega_low_frequency = 500
            
            if (i - self.last_adaptation_index >= mega_low_frequency or 
                i == self.period or 
                len(self.regime_states) == 0):
                
                # Identify market regime
                regime = self._calculate_market_regime(window_data)
                
                # Calculate volatility factor  
                volatility_factor = self._calculate_volatility_factor(window_data)
                
                # Update last adaptation index
                self.last_adaptation_index = i
                
                # Store market states
                if len(window_data) >= 21:
                    price_changes = np.diff(window_data[-20:]) / window_data[-20:-1]
                    self.volatility_states.append(np.std(price_changes))
                else:
                    self.volatility_states.append(0.01)
                self.regime_states.append(regime)
            else:
                # Use cached values for non-adaptation intervals
                regime = self.regime_states[-1] if self.regime_states else 'ranging'
                volatility_factor = 1.0  # Use default factor
              # Minimize parameter optimization to extreme low frequency
            optimize_frequency = max(self.adaptation_period, 1000)  # At least every 1000 iterations
            if (i % optimize_frequency == 0 or 
                regime not in self.parameter_sets or 
                len(self.parameter_sets) == 0):
                optimized_params = self._optimize_parameters(window_data, regime)
                
                # Apply volatility scaling
                scaled_params = self._apply_volatility_scaling(optimized_params, volatility_factor)
                
                # Update parameters with adaptation rate
                self.current_parameters = self._blend_parameters(
                    self.current_parameters, scaled_params, self.adaptation_rate
                )
                
                # Calculate performance for current parameters
                indicator_values = self._calculate_base_indicator(
                    window_data[-self.optimization_window:], self.current_parameters
                )
                performance = self._calculate_performance_score(
                    window_data[-self.optimization_window:], indicator_values
                )
                
                # Update parameter sets
                self._update_parameter_sets(regime, self.current_parameters, performance)
                
                # Store parameter history
                self.parameter_history.append(self.current_parameters.copy())
                if len(self.parameter_history) > self.performance_memory:
                    self.parameter_history.pop(0)
            
            # Calculate indicator value with current parameters
            indicator_value = self._calculate_base_indicator(
                window_data, self.current_parameters
            )[-1]
            
            # Calculate metrics
            adaptation_score = len(self.parameter_sets) / 5.0  # Max 5 regimes
            parameter_stability = self._calculate_parameter_stability(self.current_parameters)
            
            performance_score = self.parameter_sets.get(regime, 
                ParameterSet('', {}, 0.0, 0.0, 0, datetime.now())
            ).performance
            
            confidence = (adaptation_score * 0.3 + 
                         parameter_stability * 0.4 + 
                         min(performance_score / 2.0, 1.0) * 0.3)
            
            # Create signal
            signal = AdaptiveSignal(
                timestamp=data.index[i],
                indicator_value=indicator_value,
                optimal_parameters=self.current_parameters.copy(),
                adaptation_score=adaptation_score,
                regime_type=regime,
                volatility_factor=volatility_factor,
                parameter_stability=parameter_stability,
                performance_score=performance_score,
                confidence=confidence
            )
            
            results.append({
                'timestamp': signal.timestamp,
                'indicator_value': signal.indicator_value,
                'adaptation_score': signal.adaptation_score,
                'regime_type': signal.regime_type,
                'volatility_factor': signal.volatility_factor,
                'parameter_stability': signal.parameter_stability,
                'performance_score': signal.performance_score,
                'confidence': signal.confidence,
                'signal_strength': 'strong' if confidence > 0.7 else 'moderate' if confidence > 0.4 else 'weak',
                'parameter_period': self.current_parameters.get('period', 14),
                'regime_count': len(self.parameter_sets)
            })
          # Update internal arrays
        if results:
            self.adaptation_scores = np.array([r['adaptation_score'] for r in results])
            self.performance_scores = np.array([r['performance_score'] for r in results])
        
        # Store result in extreme cache before returning
        result_df = pd.DataFrame(results)
        
        # Cache management - limit cache size
        if len(self.result_cache) > 20:  # Keep only 20 most recent results
            oldest_keys = list(self.result_cache.keys())[:5]
            for key in oldest_keys:
                del self.result_cache[key]
        
        self.result_cache[cache_key] = result_df.copy()
        
        return result_df
    
    def _apply_volatility_scaling(self, params: Dict[str, float], 
                                volatility_factor: float) -> Dict[str, float]:
        """Apply volatility-based parameter scaling."""
        scaled_params = params.copy()
        
        # Scale period-based parameters
        if 'period' in scaled_params:
            scaled_params['period'] = max(5, int(scaled_params['period'] / volatility_factor))
        
        if 'fast' in scaled_params:
            scaled_params['fast'] = max(3, int(scaled_params['fast'] / volatility_factor))
        
        if 'slow' in scaled_params:
            scaled_params['slow'] = max(10, int(scaled_params['slow'] / volatility_factor))
        
        # Scale threshold parameters
        if 'std_dev' in scaled_params:
            scaled_params['std_dev'] = scaled_params['std_dev'] * volatility_factor
        
        return scaled_params
    
    def _blend_parameters(self, current: Dict[str, float], new: Dict[str, float], 
                         rate: float) -> Dict[str, float]:
        """Blend current and new parameters with adaptation rate."""
        blended = {}
        
        for key in new.keys():
            if key in current:
                blended[key] = current[key] * (1 - rate) + new[key] * rate
            else:
                blended[key] = new[key]
        
        return blended
    
    def get_signals(self, data: pd.DataFrame) -> List[AdaptiveSignal]:
        """Get adaptive indicator signals."""
        df = self.calculate(data)
        signals = []
        
        for _, row in df.iterrows():
            signal = AdaptiveSignal(
                timestamp=row['timestamp'],
                indicator_value=row['indicator_value'],
                optimal_parameters={'period': row['parameter_period']},
                adaptation_score=row['adaptation_score'],
                regime_type=row['regime_type'],
                volatility_factor=row['volatility_factor'],
                parameter_stability=row['parameter_stability'],
                performance_score=row['performance_score'],
                confidence=row['confidence']
            )
            signals.append(signal)
        
        return signals
    
    def get_parameter_analysis(self) -> Dict[str, Any]:
        """Get comprehensive parameter analysis."""
        analysis = {
            'current_parameters': self.current_parameters,
            'regime_count': len(self.parameter_sets),
            'parameter_sets': {
                regime: {
                    'parameters': ps.parameters,
                    'performance': ps.performance,
                    'usage_count': ps.usage_count
                }
                for regime, ps in self.parameter_sets.items()
            },
            'adaptation_efficiency': np.mean(self.adaptation_scores) if len(self.adaptation_scores) > 0 else 0.0,
            'average_performance': np.mean(self.performance_scores) if len(self.performance_scores) > 0 else 0.0
        }
        
        return analysis
    
    def __str__(self) -> str:
        return f"AdaptiveIndicators(base={self.base_indicator}, period={self.period})"
    
    def generate_signal(self) -> AdaptiveSignal:
        """Generate adaptive signal based on current market conditions."""
        if not hasattr(self, '_last_result') or self._last_result is None:
            # Return a default signal if no calculation has been performed
            return AdaptiveSignal(
                timestamp=datetime.now(),
                indicator_value=0.0,
                optimal_parameters=self.current_parameters,
                adaptation_score=0.0,
                regime_type="unknown",
                volatility_factor=1.0,
                parameter_stability=1.0,
                performance_score=0.0,
                confidence=0.0
            )
        
        # Calculate volatility factor
        volatility_factor = self.volatility_states[-1] if self.volatility_states else 1.0
        
        # Calculate parameter stability
        stability = 1.0
        if len(self.parameter_history) > 1:
            recent_params = self.parameter_history[-5:]
            if len(recent_params) > 1:
                param_values = [list(p.values()) for p in recent_params]
                param_std = np.std(param_values, axis=0).mean()
                stability = max(0.0, 1.0 - param_std)
        
        # Calculate performance score
        performance = np.mean(self.performance_scores[-10:]) if self.performance_scores else 0.5
        
        # Calculate adaptation score
        adaptation = np.mean(self.adaptation_scores[-5:]) if self.adaptation_scores else 0.5
        
        # Determine regime
        regime = self.regime_states[-1] if self.regime_states else "neutral"
        
        # Calculate confidence based on multiple factors
        confidence = (stability * 0.3 + performance * 0.4 + adaptation * 0.3)
        
        return AdaptiveSignal(
            timestamp=datetime.now(),
            indicator_value=float(self._last_result.value) if hasattr(self._last_result, 'value') else 0.0,
            optimal_parameters=self.current_parameters.copy(),
            adaptation_score=adaptation,
            regime_type=regime,
            volatility_factor=volatility_factor,
            parameter_stability=stability,
            performance_score=performance,
            confidence=min(1.0, max(0.0, confidence))
        )
    
    # ===== PHASE 2: ROBUSTNESS AND ERROR HANDLING METHODS =====
    
    def _validate_input_data(self, data: pd.DataFrame) -> bool:
        """
        P2.1.4: Comprehensive input data validation.
        
        Args:
            data: Input DataFrame to validate
            
        Returns:
            bool: True if data is valid, False otherwise
        """
        try:
            # Check if data is DataFrame
            if not isinstance(data, pd.DataFrame):
                return False
            
            # Check required columns
            required_columns = ['close']
            if not all(col in data.columns for col in required_columns):
                return False
            
            # Check data length
            if len(data) == 0:
                return False
            
            # Check for all NaN columns
            if data['close'].isna().all():
                return False
                
            return True
            
        except Exception:
            return False
    
    def _contains_invalid_data(self, data: pd.DataFrame) -> bool:
        """
        P2.1.1: Detect NaN and invalid data comprehensively.
        
        Args:
            data: DataFrame to check
            
        Returns:
            bool: True if data contains invalid values
        """
        try:
            close_data = data['close']
            
            # Check for NaN values
            if close_data.isna().any():
                return True
            
            # Check for infinite values
            if np.isinf(close_data).any():
                return True
            
            # Check for zero or negative prices
            if (close_data <= 0).any():
                return True
            
            # Check for extreme outliers (>10x price jumps)
            if len(close_data) > 1:
                price_changes = np.abs(np.diff(close_data) / close_data[:-1])
                if (price_changes > 10.0).any():
                    return True
            
            return False
            
        except Exception:
            return True
    
    def _clean_input_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        P2.1.2: Clean data with forward-fill and interpolation strategies.
        
        Args:
            data: DataFrame to clean
            
        Returns:
            Cleaned DataFrame or None if cleaning fails
        """
        try:
            cleaned_data = data.copy()
            close_col = 'close'
            
            # P2.1.2: Forward-fill strategy for small gaps
            cleaned_data[close_col] = cleaned_data[close_col].fillna(method='ffill')
            
            # Backward-fill for remaining NaNs at start
            cleaned_data[close_col] = cleaned_data[close_col].fillna(method='bfill')
            
            # P2.1.3: Fallback mechanism - use median for remaining NaNs
            if cleaned_data[close_col].isna().any():
                median_value = cleaned_data[close_col].median()
                if not np.isnan(median_value):
                    cleaned_data[close_col] = cleaned_data[close_col].fillna(median_value)
                else:
                    # Ultimate fallback - use a reasonable default
                    cleaned_data[close_col] = cleaned_data[close_col].fillna(100.0)
            
            # Handle infinite values
            cleaned_data[close_col] = cleaned_data[close_col].replace([np.inf, -np.inf], np.nan)
            cleaned_data[close_col] = cleaned_data[close_col].fillna(method='ffill').fillna(100.0)
            
            # Handle zero/negative prices
            invalid_prices = cleaned_data[close_col] <= 0
            if invalid_prices.any():
                # Replace with previous valid price or median
                median_price = cleaned_data[close_col][cleaned_data[close_col] > 0].median()
                if not np.isnan(median_price):
                    cleaned_data.loc[invalid_prices, close_col] = median_price
                else:
                    cleaned_data.loc[invalid_prices, close_col] = 100.0
            
            # Smooth extreme outliers
            if len(cleaned_data) > 2:
                prices = cleaned_data[close_col].values
                for i in range(1, len(prices)):
                    price_change = abs(prices[i] - prices[i-1]) / prices[i-1]
                    if price_change > 0.5:  # >50% change is suspicious
                        # Use interpolated value
                        if i < len(prices) - 1:
                            cleaned_data.iloc[i, cleaned_data.columns.get_loc(close_col)] = (prices[i-1] + prices[i+1]) / 2
                        else:
                            cleaned_data.iloc[i, cleaned_data.columns.get_loc(close_col)] = prices[i-1]
            
            return cleaned_data
            
        except Exception:
            # P2.1.3: Fallback mechanism - return None if cleaning fails
            return None
    
    def _safe_numerical_operation(self, operation_func, *args, fallback_value=0.0, operation_name="unknown"):
        """
        P2.2.1: Safe wrapper for numerical operations with exception handling.
        
        Args:
            operation_func: Function to execute safely
            *args: Arguments for the function
            fallback_value: Value to return if operation fails
            operation_name: Name of operation for logging
            
        Returns:
            Result of operation or fallback value
        """
        try:
            result = operation_func(*args)
            
            # Check if result is valid
            if np.isnan(result) or np.isinf(result):
                return fallback_value
                
            return result
            
        except (ZeroDivisionError, ValueError, RuntimeWarning, FloatingPointError):
            # P2.2.2: Graceful degradation - return safe fallback
            return fallback_value
        except Exception:
            # P2.2.2: Graceful degradation for unexpected errors
            return fallback_value
    
    def _validate_parameters(self, params: Dict[str, float]) -> Dict[str, float]:
        """
        P2.3.1: Validate and sanitize parameters with range checking.
        
        Args:
            params: Parameters to validate
            
        Returns:
            Validated and sanitized parameters
        """
        try:
            sanitized_params = {}
            
            for key, value in params.items():
                # P2.3.4: Data type enforcement
                if not isinstance(value, (int, float)):
                    try:
                        value = float(value)
                    except (ValueError, TypeError):
                        value = self._get_default_param_value(key)
                
                # P2.3.3: Bounds checking for mathematical operations
                if key == 'period':
                    sanitized_params[key] = max(1, min(int(value), 200))  # 1-200 range
                elif key in ['fast', 'slow', 'signal']:
                    sanitized_params[key] = max(1, min(int(value), 100))  # 1-100 range
                elif key in ['std_dev', 'multiplier']:
                    sanitized_params[key] = max(0.1, min(value, 10.0))  # 0.1-10.0 range
                elif key in ['overbought', 'oversold']:
                    sanitized_params[key] = max(0, min(value, 100))  # 0-100 range
                else:
                    # Default range for unknown parameters
                    sanitized_params[key] = max(0.01, min(value, 1000.0))
            
            return sanitized_params
            
        except Exception:
            # P2.2.2: Graceful degradation - return safe defaults
            return self._get_default_parameters()
    
    def _get_default_param_value(self, param_name: str) -> float:
        """Get safe default value for a parameter."""
        defaults = {
            'period': 14,
            'fast': 12,
            'slow': 26,
            'signal': 9,
            'std_dev': 2.0,
            'multiplier': 2.0,
            'overbought': 70,
            'oversold': 30
        }
        return defaults.get(param_name, 1.0)
