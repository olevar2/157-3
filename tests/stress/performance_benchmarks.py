"""
üìä PERFORMANCE BENCHMARKING AND REPORTING FOR PLATFORM3
======================================================

Automated performance benchmarking and reporting system for the
Multi-Agent Stress Testing Suite. Provides comprehensive analysis
and visualization of stress test results.

Features:
- Performance trend analysis
- Benchmark comparisons against industry standards
- Detailed reporting with charts and graphs
- Historical performance tracking
- Automated recommendations for optimization
- Export capabilities for various formats

Mission: Ensure Platform3 meets humanitarian trading performance requirements
"""

import json
import csv
import logging
import statistics
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from pathlib import Path
import traceback

@dataclass
class PerformanceBenchmark:
    """Industry standard performance benchmarks"""
    max_latency_ms: float = 50.0
    min_availability_percent: float = 99.9
    max_memory_usage_mb: float = 2048.0
    max_cpu_usage_percent: float = 80.0
    min_throughput_ops_per_second: float = 100.0
    max_error_rate_percent: float = 0.1

@dataclass
class PerformanceMetric:
    """Individual performance metric"""
    name: str
    value: float
    unit: str
    benchmark: float
    passes_benchmark: bool
    improvement_percent: float = 0.0

@dataclass
class BenchmarkReport:
    """Comprehensive benchmark report"""
    test_timestamp: datetime
    overall_score: float  # 0-100
    passes_all_benchmarks: bool
    total_operations: int
    test_duration_seconds: float
    performance_metrics: List[PerformanceMetric]
    recommendations: List[str]
    historical_comparison: Optional[Dict[str, Any]] = None

class PerformanceBenchmarker:
    """
    üìä COMPREHENSIVE PERFORMANCE BENCHMARKING SYSTEM
    
    Analyzes stress test results against industry benchmarks and
    provides detailed reports for optimization recommendations.
    """
    
    def __init__(self, benchmark_standards: Optional[PerformanceBenchmark] = None):
        self.logger = logging.getLogger(__name__)
        self.benchmark_standards = benchmark_standards or PerformanceBenchmark()
        self.historical_data: List[Dict[str, Any]] = []
        self.reports_directory = Path("performance_reports")
        self.reports_directory.mkdir(exist_ok=True)
        
        self.logger.info("üìä PerformanceBenchmarker initialized")
    
    def analyze_stress_test_results(self, stress_test_results: Dict[str, Any]) -> BenchmarkReport:
        """
        üéØ ANALYZE STRESS TEST RESULTS AGAINST BENCHMARKS
        
        Main entry point for performance analysis. Takes stress test results
        and generates comprehensive benchmark report.
        """
        self.logger.info("üìä Analyzing stress test results against benchmarks")
        
        try:
            # Extract summary data
            summary = stress_test_results.get("summary", {})
            if not summary:
                raise ValueError("No summary data found in stress test results")
            
            # Calculate overall metrics
            overall_metrics = self._calculate_overall_metrics(stress_test_results)
            
            # Analyze individual test performance
            test_metrics = self._analyze_individual_tests(stress_test_results)
            
            # Generate performance recommendations
            recommendations = self._generate_recommendations(overall_metrics, test_metrics)
            
            # Calculate overall score
            overall_score = self._calculate_overall_score(overall_metrics)
            
            # Check if all benchmarks pass
            passes_all = all(metric.passes_benchmark for metric in overall_metrics)
            
            # Load historical data for comparison
            historical_comparison = self._compare_with_historical_data(overall_metrics)
            
            # Create benchmark report
            report = BenchmarkReport(
                test_timestamp=datetime.now(),
                overall_score=overall_score,
                passes_all_benchmarks=passes_all,
                total_operations=summary.get("total_operations", 0),
                test_duration_seconds=self._calculate_total_test_duration(stress_test_results),
                performance_metrics=overall_metrics,
                recommendations=recommendations,
                historical_comparison=historical_comparison
            )
            
            self.logger.info(f"‚úÖ Benchmark analysis completed: {overall_score:.1f}/100 score")
            
            return report
            
        except Exception as e:
            self.logger.error(f"‚ùå Benchmark analysis failed: {e}")
            self.logger.error(traceback.format_exc())
            raise
    
    def _calculate_overall_metrics(self, results: Dict[str, Any]) -> List[PerformanceMetric]:
        """Calculate overall performance metrics across all tests"""
        summary = results.get("summary", {})
        individual_tests = [k for k in results.keys() if k != "summary"]
        
        metrics = []
        
        # Average latency across all tests
        latencies = []
        for test_name in individual_tests:
            test_result = results.get(test_name)
            if hasattr(test_result, 'average_latency_ms'):
                latencies.append(test_result.average_latency_ms)
        
        if latencies:
            avg_latency = statistics.mean(latencies)
            latency_metric = PerformanceMetric(
                name="Average Latency",
                value=avg_latency,
                unit="ms",
                benchmark=self.benchmark_standards.max_latency_ms,
                passes_benchmark=avg_latency <= self.benchmark_standards.max_latency_ms
            )
            metrics.append(latency_metric)
        
        # Overall availability
        availability = summary.get("overall_availability_percentage", 0)
        availability_metric = PerformanceMetric(
            name="System Availability",
            value=availability,
            unit="%",
            benchmark=self.benchmark_standards.min_availability_percent,
            passes_benchmark=availability >= self.benchmark_standards.min_availability_percent
        )
        metrics.append(availability_metric)
        
        # Peak memory usage across all tests
        memory_usages = []
        for test_name in individual_tests:
            test_result = results.get(test_name)
            if hasattr(test_result, 'peak_memory_mb'):
                memory_usages.append(test_result.peak_memory_mb)
        
        if memory_usages:
            peak_memory = max(memory_usages)
            memory_metric = PerformanceMetric(
                name="Peak Memory Usage",
                value=peak_memory,
                unit="MB",
                benchmark=self.benchmark_standards.max_memory_usage_mb,
                passes_benchmark=peak_memory <= self.benchmark_standards.max_memory_usage_mb
            )
            metrics.append(memory_metric)
        
        # Peak CPU usage across all tests
        cpu_usages = []
        for test_name in individual_tests:
            test_result = results.get(test_name)
            if hasattr(test_result, 'peak_cpu_percent'):
                cpu_usages.append(test_result.peak_cpu_percent)
        
        if cpu_usages:
            peak_cpu = max(cpu_usages)
            cpu_metric = PerformanceMetric(
                name="Peak CPU Usage",
                value=peak_cpu,
                unit="%",
                benchmark=self.benchmark_standards.max_cpu_usage_percent,
                passes_benchmark=peak_cpu <= self.benchmark_standards.max_cpu_usage_percent
            )
            metrics.append(cpu_metric)
        
        # Throughput calculation
        total_ops = summary.get("total_operations", 0)
        total_duration = self._calculate_total_test_duration(results)
        if total_duration > 0:
            throughput = total_ops / total_duration
            throughput_metric = PerformanceMetric(
                name="Operations Throughput",
                value=throughput,
                unit="ops/sec",
                benchmark=self.benchmark_standards.min_throughput_ops_per_second,
                passes_benchmark=throughput >= self.benchmark_standards.min_throughput_ops_per_second
            )
            metrics.append(throughput_metric)
        
        # Error rate calculation
        total_successful = summary.get("total_successful", 0)
        error_rate = ((total_ops - total_successful) / total_ops) * 100 if total_ops > 0 else 0
        error_metric = PerformanceMetric(
            name="Error Rate",
            value=error_rate,
            unit="%",
            benchmark=self.benchmark_standards.max_error_rate_percent,
            passes_benchmark=error_rate <= self.benchmark_standards.max_error_rate_percent
        )
        metrics.append(error_metric)
        
        return metrics
    
    def _analyze_individual_tests(self, results: Dict[str, Any]) -> Dict[str, List[PerformanceMetric]]:
        """Analyze performance of individual test scenarios"""
        individual_tests = [k for k in results.keys() if k != "summary"]
        test_metrics = {}
        
        for test_name in individual_tests:
            test_result = results.get(test_name)
            if not hasattr(test_result, 'average_latency_ms'):
                continue
                
            metrics = []
            
            # Latency metrics
            latency_metric = PerformanceMetric(
                name=f"{test_name}_latency",
                value=test_result.average_latency_ms,
                unit="ms",
                benchmark=self.benchmark_standards.max_latency_ms,
                passes_benchmark=test_result.average_latency_ms <= self.benchmark_standards.max_latency_ms
            )
            metrics.append(latency_metric)
            
            # Availability metrics
            availability_metric = PerformanceMetric(
                name=f"{test_name}_availability",
                value=test_result.availability_percentage,
                unit="%",
                benchmark=self.benchmark_standards.min_availability_percent,
                passes_benchmark=test_result.availability_percentage >= self.benchmark_standards.min_availability_percent
            )
            metrics.append(availability_metric)
            
            test_metrics[test_name] = metrics
        
        return test_metrics
    
    def _generate_recommendations(self, overall_metrics: List[PerformanceMetric], 
                                test_metrics: Dict[str, List[PerformanceMetric]]) -> List[str]:
        """Generate optimization recommendations based on performance analysis"""
        recommendations = []
        
        for metric in overall_metrics:
            if not metric.passes_benchmark:
                if metric.name == "Average Latency":
                    recommendations.append(
                        f"‚ö° Optimize latency: Current {metric.value:.1f}ms exceeds "
                        f"benchmark {metric.benchmark:.1f}ms. Consider caching, "
                        "connection pooling, or algorithm optimization."
                    )
                elif metric.name == "System Availability":
                    recommendations.append(
                        f"üîÑ Improve availability: Current {metric.value:.2f}% below "
                        f"benchmark {metric.benchmark:.2f}%. Implement redundancy, "
                        "failover mechanisms, and error handling improvements."
                    )
                elif metric.name == "Peak Memory Usage":
                    recommendations.append(
                        f"üíæ Optimize memory usage: Current {metric.value:.1f}MB exceeds "
                        f"benchmark {metric.benchmark:.1f}MB. Consider memory pooling, "
                        "garbage collection tuning, or data structure optimization."
                    )
                elif metric.name == "Peak CPU Usage":
                    recommendations.append(
                        f"‚öôÔ∏è Optimize CPU usage: Current {metric.value:.1f}% exceeds "
                        f"benchmark {metric.benchmark:.1f}%. Consider algorithm optimization, "
                        "parallel processing, or workload distribution."
                    )
                elif metric.name == "Operations Throughput":
                    recommendations.append(
                        f"üöÄ Improve throughput: Current {metric.value:.1f} ops/sec below "
                        f"benchmark {metric.benchmark:.1f} ops/sec. Consider async processing, "
                        "batching, or horizontal scaling."
                    )
                elif metric.name == "Error Rate":
                    recommendations.append(
                        f"üîß Reduce error rate: Current {metric.value:.2f}% exceeds "
                        f"benchmark {metric.benchmark:.2f}%. Improve error handling, "
                        "input validation, and system stability."
                    )
        
        # Add specific test recommendations
        for test_name, metrics in test_metrics.items():
            failing_metrics = [m for m in metrics if not m.passes_benchmark]
            if failing_metrics:
                if test_name == "high_frequency_trading":
                    recommendations.append(
                        "üìà High-frequency trading optimization needed: Consider message "
                        "batching, in-memory processing, and reduced I/O operations."
                    )
                elif test_name == "network_latency_impact":
                    recommendations.append(
                        "üåê Network resilience improvement needed: Implement retry mechanisms, "
                        "circuit breakers, and local caching for network operations."
                    )
                elif test_name == "cascading_failures":
                    recommendations.append(
                        "‚ö° Failure recovery enhancement needed: Improve isolation, "
                        "implement bulkheads, and enhance graceful degradation."
                    )
                elif test_name == "resource_stress":
                    recommendations.append(
                        "üíª Resource management optimization needed: Implement resource "
                        "limits, monitoring, and dynamic scaling capabilities."
                    )
                elif test_name == "data_consistency":
                    recommendations.append(
                        "üîÑ Data consistency improvement needed: Consider eventual consistency, "
                        "conflict resolution, and distributed locking mechanisms."
                    )
        
        # General recommendations if all tests pass
        if not recommendations:
            recommendations.extend([
                "‚úÖ All benchmarks passed! System is production-ready.",
                "üîç Consider monitoring in production to maintain performance levels.",
                "üìä Regular performance testing recommended to catch regressions early."
            ])
        
        return recommendations
    
    def _calculate_overall_score(self, metrics: List[PerformanceMetric]) -> float:
        """Calculate overall performance score (0-100)"""
        if not metrics:
            return 0.0
        
        total_score = 0.0
        for metric in metrics:
            if metric.passes_benchmark:
                # Full points for passing benchmark
                score = 100.0
            else:
                # Partial points based on how close to benchmark
                if metric.name in ["Average Latency", "Peak Memory Usage", "Peak CPU Usage", "Error Rate"]:
                    # Lower is better metrics
                    ratio = metric.benchmark / metric.value if metric.value > 0 else 0
                    score = min(100.0, ratio * 100.0)
                else:
                    # Higher is better metrics
                    ratio = metric.value / metric.benchmark if metric.benchmark > 0 else 0
                    score = min(100.0, ratio * 100.0)
            
            total_score += score
        
        return total_score / len(metrics)
    
    def _calculate_total_test_duration(self, results: Dict[str, Any]) -> float:
        """Calculate total duration of all tests"""
        individual_tests = [k for k in results.keys() if k != "summary"]
        total_duration = 0.0
        
        for test_name in individual_tests:
            test_result = results.get(test_name)
            if hasattr(test_result, 'start_time') and hasattr(test_result, 'end_time'):
                duration = (test_result.end_time - test_result.start_time).total_seconds()
                total_duration += duration
        
        return total_duration
    
    def _compare_with_historical_data(self, current_metrics: List[PerformanceMetric]) -> Optional[Dict[str, Any]]:
        """Compare current results with historical performance data"""
        if not self.historical_data:
            return None
        
        # Load latest historical data
        latest_historical = self.historical_data[-1] if self.historical_data else None
        if not latest_historical:
            return None
        
        comparison = {
            "has_historical_data": True,
            "improvements": [],
            "regressions": []
        }
        
        for metric in current_metrics:
            historical_value = latest_historical.get(metric.name.lower().replace(" ", "_"))
            if historical_value is not None:
                if metric.name in ["Average Latency", "Peak Memory Usage", "Peak CPU Usage", "Error Rate"]:
                    # Lower is better
                    if metric.value < historical_value:
                        improvement = ((historical_value - metric.value) / historical_value) * 100
                        comparison["improvements"].append(f"{metric.name}: {improvement:.1f}% better")
                    elif metric.value > historical_value:
                        regression = ((metric.value - historical_value) / historical_value) * 100
                        comparison["regressions"].append(f"{metric.name}: {regression:.1f}% worse")
                else:
                    # Higher is better
                    if metric.value > historical_value:
                        improvement = ((metric.value - historical_value) / historical_value) * 100
                        comparison["improvements"].append(f"{metric.name}: {improvement:.1f}% better")
                    elif metric.value < historical_value:
                        regression = ((historical_value - metric.value) / historical_value) * 100
                        comparison["regressions"].append(f"{metric.name}: {regression:.1f}% worse")
        
        return comparison
    
    def generate_detailed_report(self, report: BenchmarkReport, output_format: str = "json") -> str:
        """
        üìã GENERATE DETAILED PERFORMANCE REPORT
        
        Creates comprehensive performance report in specified format
        """
        self.logger.info(f"üìã Generating detailed report in {output_format} format")
        
        timestamp_str = report.test_timestamp.strftime("%Y%m%d_%H%M%S")
        
        if output_format.lower() == "json":
            return self._generate_json_report(report, timestamp_str)
        elif output_format.lower() == "csv":
            return self._generate_csv_report(report, timestamp_str)
        elif output_format.lower() == "html":
            return self._generate_html_report(report, timestamp_str)
        else:
            raise ValueError(f"Unsupported output format: {output_format}")
    
    def _generate_json_report(self, report: BenchmarkReport, timestamp_str: str) -> str:
        """Generate JSON format report"""
        filename = self.reports_directory / f"performance_report_{timestamp_str}.json"
        
        report_data = {
            "test_timestamp": report.test_timestamp.isoformat(),
            "overall_score": report.overall_score,
            "passes_all_benchmarks": report.passes_all_benchmarks,
            "total_operations": report.total_operations,
            "test_duration_seconds": report.test_duration_seconds,
            "performance_metrics": [
                {
                    "name": metric.name,
                    "value": metric.value,
                    "unit": metric.unit,
                    "benchmark": metric.benchmark,
                    "passes_benchmark": metric.passes_benchmark,
                    "improvement_percent": metric.improvement_percent
                }
                for metric in report.performance_metrics
            ],
            "recommendations": report.recommendations,
            "historical_comparison": report.historical_comparison
        }
        
        with open(filename, 'w') as f:
            json.dump(report_data, f, indent=2)
        
        self.logger.info(f"üìÑ JSON report saved to {filename}")
        return str(filename)
    
    def _generate_csv_report(self, report: BenchmarkReport, timestamp_str: str) -> str:
        """Generate CSV format report"""
        filename = self.reports_directory / f"performance_metrics_{timestamp_str}.csv"
        
        with open(filename, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Metric Name', 'Value', 'Unit', 'Benchmark', 'Passes Benchmark', 'Status'])
            
            for metric in report.performance_metrics:
                status = "PASS" if metric.passes_benchmark else "FAIL"
                writer.writerow([
                    metric.name, 
                    metric.value, 
                    metric.unit, 
                    metric.benchmark, 
                    metric.passes_benchmark,
                    status
                ])
        
        self.logger.info(f"üìä CSV report saved to {filename}")
        return str(filename)
    
    def _generate_html_report(self, report: BenchmarkReport, timestamp_str: str) -> str:
        """Generate HTML format report with charts"""
        filename = self.reports_directory / f"performance_report_{timestamp_str}.html"
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Platform3 Performance Report - {timestamp_str}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f8ff; padding: 20px; border-radius: 5px; }}
                .metric {{ margin: 10px 0; padding: 10px; border: 1px solid #ddd; border-radius: 3px; }}
                .pass {{ background-color: #d4edda; }}
                .fail {{ background-color: #f8d7da; }}
                .recommendations {{ background-color: #fff3cd; padding: 15px; border-radius: 5px; }}
                .score {{ font-size: 24px; font-weight: bold; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>üè• Platform3 Performance Report</h1>
                <p>Test Date: {report.test_timestamp.strftime('%Y-%m-%d %H:%M:%S')}</p>
                <div class="score">Overall Score: {report.overall_score:.1f}/100</div>
                <p>Status: {'‚úÖ All Benchmarks Passed' if report.passes_all_benchmarks else '‚ö†Ô∏è Some Benchmarks Failed'}</p>
            </div>
            
            <h2>üìä Performance Metrics</h2>
        """
        
        for metric in report.performance_metrics:
            status_class = "pass" if metric.passes_benchmark else "fail"
            status_icon = "‚úÖ" if metric.passes_benchmark else "‚ùå"
            
            html_content += f"""
            <div class="metric {status_class}">
                <strong>{status_icon} {metric.name}</strong><br>
                Value: {metric.value:.2f} {metric.unit}<br>
                Benchmark: {metric.benchmark:.2f} {metric.unit}<br>
                Status: {'PASS' if metric.passes_benchmark else 'FAIL'}
            </div>
            """
        
        html_content += """
            <h2>üí° Recommendations</h2>
            <div class="recommendations">
                <ul>
        """
        
        for recommendation in report.recommendations:
            html_content += f"<li>{recommendation}</li>"
        
        html_content += """
                </ul>
            </div>
        """
        
        if report.historical_comparison:
            html_content += """
                <h2>üìà Historical Comparison</h2>
                <div class="recommendations">
            """
            
            if report.historical_comparison.get("improvements"):
                html_content += "<h3>Improvements:</h3><ul>"
                for improvement in report.historical_comparison["improvements"]:
                    html_content += f"<li>‚úÖ {improvement}</li>"
                html_content += "</ul>"
            
            if report.historical_comparison.get("regressions"):
                html_content += "<h3>Regressions:</h3><ul>"
                for regression in report.historical_comparison["regressions"]:
                    html_content += f"<li>‚ö†Ô∏è {regression}</li>"
                html_content += "</ul>"
            
            html_content += "</div>"
        
        html_content += """
        </body>
        </html>
        """
        
        with open(filename, 'w') as f:
            f.write(html_content)
        
        self.logger.info(f"üåê HTML report saved to {filename}")
        return str(filename)
    
    def create_performance_charts(self, report: BenchmarkReport) -> List[str]:
        """
        üìà CREATE PERFORMANCE VISUALIZATION CHARTS
        
        Generates various charts for performance analysis
        """
        self.logger.info("üìà Creating performance visualization charts")
        
        chart_files = []
        timestamp_str = report.test_timestamp.strftime("%Y%m%d_%H%M%S")
        
        try:
            # Metrics vs Benchmarks Chart
            chart_file = self._create_metrics_comparison_chart(report, timestamp_str)
            chart_files.append(chart_file)
            
            # Performance Score Chart
            chart_file = self._create_score_chart(report, timestamp_str)
            chart_files.append(chart_file)
            
            # Historical Trend Chart (if historical data available)
            if report.historical_comparison and report.historical_comparison.get("has_historical_data"):
                chart_file = self._create_historical_trend_chart(report, timestamp_str)
                chart_files.append(chart_file)
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Chart generation failed: {e}")
        
        return chart_files
    
    def _create_metrics_comparison_chart(self, report: BenchmarkReport, timestamp_str: str) -> str:
        """Create metrics vs benchmarks comparison chart"""
        filename = self.reports_directory / f"metrics_comparison_{timestamp_str}.png"
        
        metrics_names = [metric.name for metric in report.performance_metrics]
        metric_values = [metric.value for metric in report.performance_metrics]
        benchmark_values = [metric.benchmark for metric in report.performance_metrics]
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        x = np.arange(len(metrics_names))
        width = 0.35
        
        bars1 = ax.bar(x - width/2, metric_values, width, label='Actual', color='skyblue')
        bars2 = ax.bar(x + width/2, benchmark_values, width, label='Benchmark', color='lightcoral')
        
        ax.set_xlabel('Performance Metrics')
        ax.set_ylabel('Values')
        ax.set_title('Performance Metrics vs Benchmarks')
        ax.set_xticks(x)
        ax.set_xticklabels(metrics_names, rotation=45, ha='right')
        ax.legend()
        
        # Add value labels on bars
        for bar in bars1:
            height = bar.get_height()
            ax.annotate(f'{height:.1f}',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3),
                       textcoords="offset points",
                       ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"üìä Metrics comparison chart saved to {filename}")
        return str(filename)
    
    def _create_score_chart(self, report: BenchmarkReport, timestamp_str: str) -> str:
        """Create performance score visualization"""
        filename = self.reports_directory / f"performance_score_{timestamp_str}.png"
        
        fig, ax = plt.subplots(figsize=(8, 8))
        
        # Create a pie chart showing score breakdown
        passing_metrics = sum(1 for metric in report.performance_metrics if metric.passes_benchmark)
        failing_metrics = len(report.performance_metrics) - passing_metrics
        
        sizes = [passing_metrics, failing_metrics]
        labels = ['Passing Benchmarks', 'Failing Benchmarks']
        colors = ['#90EE90', '#FFB6C1']
        
        wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        
        ax.set_title(f'Performance Score: {report.overall_score:.1f}/100', fontsize=16, fontweight='bold')
        
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"üéØ Performance score chart saved to {filename}")
        return str(filename)
    
    def _create_historical_trend_chart(self, report: BenchmarkReport, timestamp_str: str) -> str:
        """Create historical performance trend chart"""
        filename = self.reports_directory / f"historical_trend_{timestamp_str}.png"
        
        # This would require actual historical data storage and retrieval
        # For now, create a placeholder chart
        fig, ax = plt.subplots(figsize=(12, 6))
        
        # Simulated historical data for demonstration
        dates = pd.date_range(end=report.test_timestamp, periods=10, freq='D')
        scores = np.random.uniform(85, 95, 10)  # Simulated scores
        scores[-1] = report.overall_score  # Current score
        
        ax.plot(dates, scores, marker='o', linewidth=2, markersize=6)
        ax.set_xlabel('Date')
        ax.set_ylabel('Performance Score')
        ax.set_title('Historical Performance Trend')
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 100)
        
        # Highlight current score
        ax.axhline(y=report.overall_score, color='red', linestyle='--', alpha=0.7, label=f'Current Score: {report.overall_score:.1f}')
        ax.legend()
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"üìà Historical trend chart saved to {filename}")
        return str(filename)
    
    def save_historical_data(self, report: BenchmarkReport):
        """Save current performance data for historical tracking"""
        historical_entry = {
            "timestamp": report.test_timestamp.isoformat(),
            "overall_score": report.overall_score,
            "passes_all_benchmarks": report.passes_all_benchmarks,
            "total_operations": report.total_operations,
            "test_duration_seconds": report.test_duration_seconds
        }
        
        # Add individual metrics
        for metric in report.performance_metrics:
            key = metric.name.lower().replace(" ", "_")
            historical_entry[key] = metric.value
        
        self.historical_data.append(historical_entry)
        
        # Save to file
        historical_file = self.reports_directory / "historical_performance_data.json"
        with open(historical_file, 'w') as f:
            json.dump(self.historical_data, f, indent=2)
        
        self.logger.info(f"üíæ Historical data saved to {historical_file}")
    
    def load_historical_data(self):
        """Load historical performance data"""
        historical_file = self.reports_directory / "historical_performance_data.json"
        if historical_file.exists():
            try:
                with open(historical_file, 'r') as f:
                    self.historical_data = json.load(f)
                self.logger.info(f"üìä Loaded {len(self.historical_data)} historical records")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Failed to load historical data: {e}")
    
    def generate_comprehensive_analysis(self, stress_test_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        üéØ GENERATE COMPREHENSIVE PERFORMANCE ANALYSIS
        
        Main entry point for complete performance analysis including
        benchmarking, reporting, and visualization.
        """
        self.logger.info("üéØ Starting comprehensive performance analysis")
        
        try:
            # Load historical data
            self.load_historical_data()
            
            # Analyze results against benchmarks
            benchmark_report = self.analyze_stress_test_results(stress_test_results)
            
            # Generate reports in multiple formats
            json_report = self.generate_detailed_report(benchmark_report, "json")
            csv_report = self.generate_detailed_report(benchmark_report, "csv")
            html_report = self.generate_detailed_report(benchmark_report, "html")
            
            # Create visualization charts
            chart_files = self.create_performance_charts(benchmark_report)
            
            # Save historical data
            self.save_historical_data(benchmark_report)
            
            # Compile comprehensive analysis
            analysis = {
                "benchmark_report": benchmark_report,
                "report_files": {
                    "json": json_report,
                    "csv": csv_report,
                    "html": html_report
                },
                "chart_files": chart_files,
                "summary": {
                    "overall_score": benchmark_report.overall_score,
                    "passes_all_benchmarks": benchmark_report.passes_all_benchmarks,
                    "total_operations": benchmark_report.total_operations,
                    "test_duration_seconds": benchmark_report.test_duration_seconds,
                    "recommendations_count": len(benchmark_report.recommendations),
                    "system_production_ready": benchmark_report.passes_all_benchmarks and benchmark_report.overall_score >= 80.0
                }
            }
            
            self.logger.info("‚úÖ Comprehensive performance analysis completed")
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå Comprehensive analysis failed: {e}")
            self.logger.error(traceback.format_exc())
            raise

# Example usage
def main():
    """Example usage of the performance benchmarker"""
    logging.basicConfig(level=logging.INFO)
    
    # Example stress test results (would come from actual stress testing)
    example_results = {
        "summary": {
            "total_operations": 5000,
            "total_successful": 4950,
            "overall_availability_percentage": 99.0,
            "system_production_ready": True
        },
        "high_frequency_trading": type('TestResult', (), {
            'average_latency_ms': 45.0,
            'availability_percentage': 99.2,
            'peak_memory_mb': 1024.0,
            'peak_cpu_percent': 75.0,
            'start_time': datetime.now() - timedelta(minutes=10),
            'end_time': datetime.now() - timedelta(minutes=5)
        })()
    }
    
    # Create benchmarker and analyze results
    benchmarker = PerformanceBenchmarker()
    analysis = benchmarker.generate_comprehensive_analysis(example_results)
    
    print(f"Analysis completed: {analysis['summary']['overall_score']:.1f}/100 score")

if __name__ == "__main__":
    main()
